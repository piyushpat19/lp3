Explanation:

pandas â†’ read and handle datasets (CSV files).

numpy â†’ mathematical calculations.

seaborn & matplotlib â†’ for data visualization (graphs).

tensorflow â†’ deep learning library to build and train Neural Networks.

ðŸ’¬ Viva Tip:

These libraries together handle data, visualization, and model training.

ðŸŸ© Cell 2 â€” Loading the Dataset
df = pd.read_csv('Churn_Modelling.csv')
df.head()


Explanation:

Loads the dataset into a DataFrame df.

df.head() displays the first 5 rows.

Columns include:
CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited

âœ… Target column: Exited
(1 â†’ Customer left the bank, 0 â†’ Customer stayed)

ðŸŸ© Cell 3 â€” Checking Dataset Information
df.info()


Explanation:

Shows number of rows, columns, data types, and missing values.

Confirms all numeric and categorical fields.

ðŸŸ© Cell 4 â€” Visualizing Target Variable
plt.xlabel('Exited')
plt.ylabel('Count')
df['Exited'].value_counts().plot.bar()
plt.show()


Explanation:

Creates a bar chart of customers who stayed vs those who left.

Helps check for class imbalance (usually more customers stay).

ðŸŸ© Cell 5 â€” Checking Categorical Columns
df['Geography'].value_counts()


Output Example:

France     5014
Germany    2509
Spain      2497


Explanation:

Shows how many customers belong to each country.

Used later for encoding categorical features into numbers.

ðŸŸ© Cell 6 â€” Data Preprocessing
df = df.drop(['RowNumber','CustomerId','Surname'], axis=1)


Explanation:

Removes unnecessary columns that donâ€™t affect churn prediction.

âœ… Remaining useful features â†’ customer info & activity columns.

ðŸŸ© Cell 7 â€” Encoding Categorical Variables
df = pd.get_dummies(df, drop_first=True)


Explanation:

Converts categorical columns (Geography, Gender) into numeric ones using One-Hot Encoding.

drop_first=True removes one dummy column to avoid redundancy.

Example:

Gender	Geography	Gender_Male	Geography_Germany	Geography_Spain
ðŸŸ© Cell 8 â€” Feature and Target Split
X = df.drop('Exited', axis=1)
y = df['Exited']


Explanation:

X â†’ all independent variables (customer data).

y â†’ dependent variable (whether they churned).

ðŸŸ© Cell 9 â€” Splitting into Train/Test Sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


Explanation:

Splits dataset:

80% â†’ training data

20% â†’ testing data

Ensures reproducibility using random_state=42.

ðŸŸ© Cell 10 â€” Normalization
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)


Explanation:

Scales numeric data to have mean = 0, std = 1.

Neural networks train faster and more accurately on scaled data.

ðŸŸ© Cell 11 â€” Building the Neural Network
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

model = Sequential()
model.add(Flatten(input_shape=(13,)))
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


Explanation:

Layer	Purpose
Flatten(13,)	Input layer with 13 features
Dense(100, ReLU)	Hidden layer with 100 neurons; learns complex patterns
Dense(1, Sigmoid)	Output layer producing a probability (0 â€“ 1) for churn
ðŸŸ© Cell 12 â€” Compiling the Model
model.compile(optimizer='adam',
              loss='BinaryCrossentropy',
              metrics=['accuracy'])


Explanation:

optimizer = 'adam' â†’ adaptive gradient descent for faster convergence.

loss = 'BinaryCrossentropy' â†’ measures how wrong predictions are in binary classification.

metrics = ['accuracy'] â†’ tracks correctness percentage.

ðŸŸ© Cell 13 â€” Training the Model
model.fit(x_train, y_train, batch_size=64, validation_split=0.1, epochs=100)


Explanation:

batch_size = 64 â†’ updates weights every 64 samples.

validation_split = 0.1 â†’ uses 10 % of training data to validate.

epochs = 100 â†’ full passes over training data.

Training output shows:

Epoch 1/100
loss: 0.45 accuracy: 0.78 val_loss: 0.41 val_accuracy: 0.82
...
Epoch 100/100
loss: 0.12 accuracy: 0.91 val_loss: 0.16 val_accuracy: 0.88


âœ… Loss decreases, accuracy increases â†’ model learning well.

ðŸŸ© Cell 14 â€” Evaluating Model
y_pred = model.predict(x_test)
y_pred = (y_pred > 0.5)
from sklearn.metrics import confusion_matrix, accuracy_score
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))


Explanation:

model.predict() â†’ predicts churn probability for test data.

Converts probabilities > 0.5 â†’ 1 (churn), else 0 (stay).

confusion_matrix â†’ shows correct/incorrect predictions.

accuracy_score â†’ final test accuracy (usually ~85â€“90 %).






imp questions
